{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImitationLearning",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRzPplAmaVs1EE3znhUbhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BingyuZhou/DRLBook/blob/master/ImitationLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVihzl3Qd-p-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cb04dc44-25bd-4bc7-dba2-caa5970acc59"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gym\n",
        "torch.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.6.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNXUmTAHfLvB",
        "colab_type": "text"
      },
      "source": [
        "## Imitation Learning (DAGGER)\n",
        "\n",
        "Given an expert policy ($\\pi_{exp}$) as black box, training a new neural network ($\\pi_\\theta$) to immitate the policy distributoin of expert.\n",
        "\n",
        "DAGGER algorithm (Dataset aggregation):\n",
        "---\n",
        "while training:\n",
        "- train $\\pi_\\theta$ from expert dataset $D= \\{o_1,a_1,...,o_n,a_n \\}$\n",
        "- run $\\pi_\\theta$ to get dataset $D_{\\pi} = \\{o_1,...,o_n \\}$\n",
        "- relabel $D_{\\pi}$ with expert policy $D_{\\pi}^{expert} = \\{o_1,a_1,...,o_n,a_n \\}$\n",
        "- $D += D_{\\pi}^{expert}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV9TshATn2Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1IWyh_neBk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, feature_dim, output_dim):\n",
        "    super(Model, self).__init__()\n",
        "    self.dense1 = nn.Linear(feature_dim, 256)\n",
        "    self.dense2 = nn.Linear(120, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = nn.functional.relu(self.dense1(x))\n",
        "    return self.dense2(x)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "  def __init__(self, feature_dim, output_dim):\n",
        "    self.mu = Model(feature_dim, output_dim)\n",
        "    log_std = -0.5*np.ones(output_dim, dtype=np.float32)\n",
        "    self.log_std = torch.nn.Parameter(torch.as_tensor(log_std)) # is this trained?\n",
        "    self.loss = nn.MSELoss()\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters)\n",
        "\n",
        "  def train(self, iters, batch_size, dataset):\n",
        "    for i in iters:\n",
        "      obs, expert_act = dataset.sample(batch_size)\n",
        "      act = self.inference(obs)\n",
        "      loss = self.loss(act, expert_act)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      for param in self.model.parameters():\n",
        "        param.grad.data.clamp(-1,1)\n",
        "      self.optimizer.step()\n",
        "  \n",
        "  def inference(self, x):\n",
        "    mean = self.mu(x)\n",
        "    return mean+torch.exp(self.log_std)*torch.normal(torch.zeros_like(mean), 1)\n",
        "\n",
        "class DataSet():\n",
        "  def __init__(self, size, obs_dim, act_dim):\n",
        "    self.observation = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "    self.action = np.zeros((size, act_dim), dtype=np.float32)\n",
        "\n",
        "    self._pos = 0\n",
        "    self._size = size\n",
        "  \n",
        "  def add(self, obs, act):\n",
        "    self.observation[self._pos,:] = obs\n",
        "    self.action[self._pos,:] =act\n",
        "    self._pos = (self._pos+1) % self._size\n",
        "  \n",
        "  def sample(self, size):\n",
        "    idxes = np.random.choice(self._pos+1, size, replace=False)\n",
        "    return self.observation[idxes], self.action[idxes]\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhD8TYmfo-mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_training_trajs(env_fn, policy, batch_size, max_ep_len):\n",
        "  traj_obs = []\n",
        "  traj_act = []\n",
        "  env = env_fn()\n",
        "  while len(traj_obs) < batch_size:\n",
        "    obs = env.reset()\n",
        "    step = 0\n",
        "    while True:\n",
        "      traj_obs.append(obs)\n",
        "      act = policy(obs)\n",
        "      traj_act.append(act)\n",
        "\n",
        "      obs, rew, done = env.step(act)\n",
        "      step+=1\n",
        "\n",
        "      if done or steps>=max_ep_len:\n",
        "        break\n",
        "  return (traj_obs, traj_act)\n",
        "\n",
        "def relabel_with_expert(trajs, expert):\n",
        "      act = expert(trajs[0])\n",
        "      trajs[1] = act\n",
        "      return trajs\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tqi7BL8eIQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DAgger(iters, env_fn, agent, expert, dataset, batch_size, max_ep_len):\n",
        "  for i in iters:\n",
        "    agent.train()\n",
        "\n",
        "    trajs = collect_training_trajs(env_fn, agent, batch_size, max_ep_len)\n",
        "    trajs = relabel_with_expert(trajs, expert)\n",
        "    dataset.add(trajs)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkOav8m2s3iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expert_policy_path = ''\n",
        "expert_policy = torch.load(expert_policy_path)\n",
        "expert_policy.eval()\n",
        "\n",
        "feature_dim = 3\n",
        "output_dim = 3\n",
        "agent = Agent(feature_dim, output_dim)\n",
        "dataset = DataSet(int(1e7), feature_dim, output_dim)\n",
        "env_fn = lambda gym.make(\"BipedalWalker-v2\")\n",
        "batch_size = 128\n",
        "max_ep_len = 200\n",
        "\n",
        "DAgger(10, env_fn, agent, expert_policy, dataset, batch_size, max_ep_len)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}